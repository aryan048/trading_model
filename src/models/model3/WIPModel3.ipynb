{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "executionInfo": {
     "elapsed": 42868,
     "status": "ok",
     "timestamp": 1743736194799,
     "user": {
      "displayName": "Aryan Hazra",
      "userId": "11477684992723650711"
     },
     "user_tz": 240
    },
    "id": "i2f2zetSfwtm",
    "outputId": "b7fbd935-be65-4552-9006-6da0eeb70e25"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from models.utils.sp_scraper import scrape_sp500_symbols\n",
    "import yfinance as yf\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dense, BatchNormalization, LeakyReLU, Dropout, Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.models import Sequential\n",
    "import talib as ta\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 266955,
     "status": "ok",
     "timestamp": 1743736461751,
     "user": {
      "displayName": "Aryan Hazra",
      "userId": "11477684992723650711"
     },
     "user_tz": 240
    },
    "id": "dJg_o59ofwto",
    "outputId": "8e6c1605-54aa-4ee7-a242-825c49f71c68"
   },
   "outputs": [],
   "source": [
    "# Replace '.' with '-' in ticker symbols, also add SPY as a benchmark\n",
    "sp_tickers = [ticker.replace(\".\", \"-\") for ticker in sorted(scrape_sp500_symbols())] + [\"^GSPC\"]\n",
    "\n",
    "scalers = {}\n",
    "data_frames = []\n",
    "\n",
    "def process_ticker(ticker):\n",
    "    try:\n",
    "        # Retry logic\n",
    "        while True:\n",
    "            try:\n",
    "                ticker_data = yf.Ticker(ticker).history(period=\"max\")\n",
    "                break\n",
    "            except Exception:\n",
    "                time.sleep(10)\n",
    "\n",
    "        if ticker_data.empty:\n",
    "            return None, None\n",
    "\n",
    "        # Process data\n",
    "        ticker_data.reset_index(inplace=True)\n",
    "        ticker_data.columns = ticker_data.columns.str.lower()\n",
    "        ticker_data['ticker'] = ticker\n",
    "        ticker_data['log_return_30d'] = np.log(ticker_data['close'].shift(-30) / ticker_data['close'])\n",
    "\n",
    "        ticker_data['rsi'] = ta.RSI(ticker_data['close'], timeperiod=14)\n",
    "        macd, macdsignal, macdhist = ta.MACD(ticker_data['close'], fastperiod=12, slowperiod=26, signalperiod=9)\n",
    "        ticker_data['macd'] = macd\n",
    "        ticker_data['sma_10'] = ta.SMA(ticker_data['close'], timeperiod=10)\n",
    "        ticker_data['sma_30'] = ta.SMA(ticker_data['close'], timeperiod=30)\n",
    "\n",
    "        # Initialize scalers\n",
    "        scaler_close = StandardScaler()\n",
    "        scaler_future_price = StandardScaler()\n",
    "        scaler_sma_10 = StandardScaler()\n",
    "        scaler_sma_30 = StandardScaler()\n",
    "\n",
    "        # Select and scale\n",
    "        close_vals = ticker_data[['close']].values\n",
    "        log_return_vals = ticker_data[['log_return_30d']].values\n",
    "        sma_10_vals = ticker_data[['sma_10']].values\n",
    "        sma_30_vals = ticker_data[['sma_30']].values\n",
    "\n",
    "        ticker_data['scaled_close'] = scaler_close.fit_transform(close_vals)\n",
    "        ticker_data['scaled_log_return_30d'] = scaler_future_price.fit_transform(log_return_vals)\n",
    "        ticker_data['scaled_sma_10'] = scaler_sma_10.fit_transform(sma_10_vals)\n",
    "        ticker_data['scaled_sma_30'] = scaler_sma_30.fit_transform(sma_30_vals)\n",
    "\n",
    "        # Save scalers\n",
    "        ticker_scalers = {\n",
    "            'scaler_close': scaler_close,\n",
    "            'scaler_future_price': scaler_future_price,\n",
    "            'scaler_sma_10': scaler_sma_10,\n",
    "            'scaler_sma_30': scaler_sma_30\n",
    "        }\n",
    "\n",
    "        return ticker_data, (ticker, ticker_scalers)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {ticker}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Use multithreading for I/O-bound operations like data download\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    futures = {executor.submit(process_ticker, ticker): ticker for ticker in sp_tickers}\n",
    "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Downloading data\", unit=\"ticker\"):\n",
    "        result_data, result_scalers = future.result()\n",
    "        if result_data is not None:\n",
    "            data_frames.append(result_data)\n",
    "        else:\n",
    "            print(f\"Failed to process {future.result()}\")\n",
    "        if result_scalers is not None:\n",
    "            ticker, scaler_dict = result_scalers\n",
    "            scalers[ticker] = scaler_dict\n",
    "        else:\n",
    "            print(f\"Failed to process {future.result()}\")\n",
    "\n",
    "# Combine all dataframes\n",
    "data = pd.concat(data_frames, ignore_index=True)\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 96,
     "status": "ok",
     "timestamp": 1743736461852,
     "user": {
      "displayName": "Aryan Hazra",
      "userId": "11477684992723650711"
     },
     "user_tz": 240
    },
    "id": "cXfl1NZ3fwtp"
   },
   "outputs": [],
   "source": [
    "# Prepare the data for the model\n",
    "# Label encode the ticker column\n",
    "label_encoder = LabelEncoder()\n",
    "data[\"encoded_ticker\"] = label_encoder.fit_transform(data[\"ticker\"])\n",
    "\n",
    "# Initialize scalers\n",
    "scaler_ticker = StandardScaler()\n",
    "scaler_technical = StandardScaler()\n",
    "\n",
    "\n",
    "# Scale the ticker column\n",
    "stock_ticker = data.filter([\"encoded_ticker\"])\n",
    "stock_ticker = stock_ticker.values\n",
    "scaled_ticker = scaler_ticker.fit_transform(stock_ticker)\n",
    "data['scaled_ticker'] = scaled_ticker\n",
    "\n",
    "#scale technical columns\n",
    "stock_technical = data.filter([\"return\", \"rsi\", \"macd\"])\n",
    "stock_technical = stock_technical.values\n",
    "scaled_technicals = scaler_technical.fit_transform(stock_technical)\n",
    "# Insert scaled data into the original dataframe\n",
    "data['scaled_rsi'] = scaled_technicals[:, 0]\n",
    "data['scaled_macd'] = scaled_technicals[:, 1]\n",
    "\n",
    "# Group the data by ticker\n",
    "grouped_dfs = data.groupby('ticker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 647844,
     "status": "ok",
     "timestamp": 1743737109698,
     "user": {
      "displayName": "Aryan Hazra",
      "userId": "11477684992723650711"
     },
     "user_tz": 240
    },
    "id": "ZDEglV1Qfwtq",
    "outputId": "6b4120d9-7479-4fca-ee87-a5f194b8e4be"
   },
   "outputs": [],
   "source": [
    "# List the features you want to include (excluding 'price in 30 days' and 'date')\n",
    "# scaled_close, scaled_sma's are scaled by ticker, rest by total df\n",
    "feature_cols = ['scaled_close', 'scaled_rsi', 'scaled_macd', 'scaled_sma_10', 'scaled_sma_30', 'scaled_ticker']  # add your actual column names here\n",
    "\n",
    "x_train, y_train = [], []\n",
    "\n",
    "for ticker, df in tqdm(grouped_dfs, desc=\"Creating sliding windows\", unit=\"ticker\"):\n",
    "    df.sort_values(by='date', inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    if len(df) < 91:\n",
    "        continue\n",
    "    \n",
    "    for i in range(60, len(df) - 30):\n",
    "        # Extract a sliding window of all desired features\n",
    "        window = df.iloc[i - 60:i][feature_cols].values  # shape (60, num_features)\n",
    "\n",
    "        # Optional: add ticker as a numeric value if it's useful\n",
    "        # ticker_id = your_ticker_encoding[ticker]  # if you're using one-hot or label encoding\n",
    "        # ticker_column = np.full((60, 1), ticker_id)\n",
    "        # window = np.hstack((window, ticker_column))\n",
    "\n",
    "        x_train.append(window)\n",
    "\n",
    "        # Predict the \"price in 30 days\" from the current i-th index (i.e. day 60 of the window)\n",
    "        y_train.append(df.iloc[i]['scaled_log_return_30d'])\n",
    "        \n",
    "# Convert to numpy arrays\n",
    "x_train, y_train = np.array(x_train), np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1124244,
     "status": "aborted",
     "timestamp": 1743737210214,
     "user": {
      "displayName": "Aryan Hazra",
      "userId": "11477684992723650711"
     },
     "user_tz": 240
    },
    "id": "xgaQ4tDHfwtq"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Convolutional layers for local pattern extraction\n",
    "model.add(Conv1D(filters=64, kernel_size=3, padding='same', activation='relu', input_shape=(x_train.shape[1], x_train.shape[2])))\n",
    "model.add(Conv1D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "# Stacked Bidirectional LSTM for capturing sequence relationships\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Dense layers for final nonlinear transformation\n",
    "model.add(Dense(128, kernel_regularizer=regularizers.l2(0.001)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dropout(0.4))  # Slightly increased dropout to reduce overfitting\n",
    "\n",
    "model.add(Dense(64, kernel_regularizer=regularizers.l2(0.001)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(1))  # Output log return prediction\n",
    "\n",
    "model.summary()\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001, clipnorm=1.0),\n",
    "    loss=keras.losses.Huber(delta=1.0),  # Huber = better for stability on noisy targets\n",
    "    metrics=[keras.metrics.RootMeanSquaredError()]\n",
    ")\n",
    "\n",
    "\n",
    "lr_schedule = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', \n",
    "                                                factor=0.5, \n",
    "                                                patience=3, \n",
    "                                                verbose=1)\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                           patience=10, \n",
    "                                           restore_best_weights=True)\n",
    "\n",
    "training = model.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=200,                # Max number of epochs\n",
    "    batch_size=32,\n",
    "    validation_split=0.1,      # Use part of training data for validation\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "model.save(\"model3.keras\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
