{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install yfinance\n",
        "!pip install tensorflow\n",
        "url = 'https://anaconda.org/conda-forge/libta-lib/0.4.0/download/linux-64/libta-lib-0.4.0-h166bdaf_1.tar.bz2'\n",
        "!curl -L $url | tar xj -C /usr/lib/x86_64-linux-gnu/ lib --strip-components=1\n",
        "!pip install conda-package-handling\n",
        "!wget https://anaconda.org/conda-forge/ta-lib/0.5.1/download/linux-64/ta-lib-0.5.1-py311h9ecbd09_0.conda\n",
        "!cph x ta-lib-0.5.1-py311h9ecbd09_0.conda\n",
        "!mv ./ta-lib-0.5.1-py311h9ecbd09_0/lib/python3.11/site-packages/talib /usr/local/lib/python3.11/dist-packages/"
      ],
      "metadata": {
        "id": "OZ6doyUaUEuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "metadata": {
        "id": "NX6PmunBbEpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2f2zetSfwtm"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sp_scraper import SPScraper\n",
        "\n",
        "import yfinance as yf\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.layers import Bidirectional, LSTM, Dense, BatchNormalization, LeakyReLU, Dropout, Conv1D, MaxPooling1D\n",
        "from tensorflow.keras.models import Sequential\n",
        "import talib as ta\n",
        "import time\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import joblib\n",
        "import os\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "from tqdm.notebook import tqdm\n",
        "import multiprocessing\n",
        "import json\n",
        "import talib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJg_o59ofwto"
      },
      "outputs": [],
      "source": [
        "# Replace '.' with '-' in ticker symbols, also add SPY as a benchmark\n",
        "scraper = SPScraper()\n",
        "\n",
        "sp_tickers = [ticker.replace(\".\", \"-\") for ticker in sorted(scraper.scrape_sp500_symbols().index)] + [\"^GSPC\"]\n",
        "scalers = {}\n",
        "data_frames = []\n",
        "\n",
        "def process_ticker(ticker):\n",
        "    try:\n",
        "        # Retry logic\n",
        "        while True:\n",
        "            try:\n",
        "                ticker_data = yf.Ticker(ticker).history(period=\"5y\")\n",
        "                break\n",
        "            except Exception:\n",
        "                time.sleep(10)\n",
        "\n",
        "        if ticker_data.empty:\n",
        "            print(f\"{ticker} is empty\")\n",
        "            return\n",
        "\n",
        "        # Process data\n",
        "        ticker_data.reset_index(inplace=True)\n",
        "        ticker_data.columns = ticker_data.columns.str.lower()\n",
        "        ticker_data['ticker'] = ticker\n",
        "\n",
        "\n",
        "        # Scale per ticker\n",
        "        ticker_data[\"macd\"], ticker_data[\"macdsignal\"], ticker_data[\"macdhist\"] = ta.MACD(ticker_data['close'], fastperiod=12, slowperiod=26, signalperiod=9)\n",
        "        ticker_data['sma_10'] = ta.SMA(ticker_data['close'], timeperiod=10)\n",
        "        ticker_data['sma_30'] = ta.SMA(ticker_data['close'], timeperiod=30)\n",
        "\n",
        "        # Scale per df\n",
        "        ticker_data['rsi'] = ta.RSI(ticker_data['close'], timeperiod=14)\n",
        "        ticker_data['log_return_30d'] = np.log(ticker_data['close'].shift(-30) / ticker_data['close'])\n",
        "        ticker_data['cdl2crows'] = ta.CDL2CROWS(ticker_data['open'], ticker_data['high'], ticker_data['low'], ticker_data['close'])\n",
        "\n",
        "        # Initialize scalers\n",
        "        scaler_close = StandardScaler()\n",
        "        scaler_macd = StandardScaler()\n",
        "        scaler_sma_10 = StandardScaler()\n",
        "        scaler_sma_30 = StandardScaler()\n",
        "\n",
        "        # Select and scale\n",
        "        close_vals = ticker_data[['close']].values\n",
        "        macd_vals = ticker_data[['macd', 'macdsignal', 'macdhist']].values\n",
        "        sma_10_vals = ticker_data[['sma_10']].values\n",
        "        sma_30_vals = ticker_data[['sma_30']].values\n",
        "\n",
        "        ticker_data['scaled_close'] = scaler_close.fit_transform(close_vals)\n",
        "        scaled_macd_values = scaler_macd.fit_transform(macd_vals)\n",
        "        ticker_data['scaled_sma_10'] = scaler_sma_10.fit_transform(sma_10_vals)\n",
        "        ticker_data['scaled_sma_30'] = scaler_sma_30.fit_transform(sma_30_vals)\n",
        "\n",
        "        # Unpack necessary scaled vals\n",
        "        ticker_data['scaled_macd'] = scaled_macd_values[:, 0]\n",
        "        ticker_data['scaled_macdsignal'] = scaled_macd_values[:, 1]\n",
        "        ticker_data['scaled_macdhist'] = scaled_macd_values[:, 2]\n",
        "\n",
        "        # Save scalers\n",
        "        ticker_scalers = {\n",
        "            'scaler_close': scaler_close,\n",
        "            'scaler_macd': scaler_macd,\n",
        "            'scaler_sma_10': scaler_sma_10,\n",
        "            'scaler_sma_30': scaler_sma_30\n",
        "        }\n",
        "\n",
        "\n",
        "        return ticker_data, (ticker, ticker_scalers)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Issue applying technical indicators to {ticker}: {e}\")\n",
        "        return\n",
        "\n",
        "# Use multithreading for I/O-bound operations like data download\n",
        "with ThreadPoolExecutor(max_workers=10) as executor:\n",
        "    futures = {executor.submit(process_ticker, ticker): ticker for ticker in sp_tickers}\n",
        "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Downloading data\", unit=\"ticker\"):\n",
        "        result_data, result_scalers = future.result()\n",
        "        if result_data is not None:\n",
        "            data_frames.append(result_data)\n",
        "        else:\n",
        "            print(f\"Failed to process {future.result()}\")\n",
        "        if result_scalers is not None:\n",
        "            ticker, scaler_dict = result_scalers\n",
        "            scalers[ticker] = scaler_dict\n",
        "        else:\n",
        "            print(f\"Failed to process {future.result()}\")\n",
        "\n",
        "# After the ThreadPoolExecutor block, add this code:\n",
        "# Save all scalers\n",
        "for ticker, scaler_dict in scalers.items():\n",
        "    # Create ticker-specific directory\n",
        "    ticker_dir = os.path.join('scalers', ticker)\n",
        "    os.makedirs(ticker_dir, exist_ok=True)\n",
        "\n",
        "    # Save each scaler in the ticker's directory\n",
        "    for scaler_name, scaler in scaler_dict.items():\n",
        "        joblib.dump(scaler, os.path.join(ticker_dir, f'{scaler_name}.pkl'))\n",
        "\n",
        "# Combine all dataframes\n",
        "data = pd.concat(data_frames, ignore_index=True)\n",
        "data.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXfl1NZ3fwtp"
      },
      "outputs": [],
      "source": [
        "# Prepare the data for the model\n",
        "# Label encode the ticker column\n",
        "label_encoder = LabelEncoder()\n",
        "data[\"encoded_ticker\"] = label_encoder.fit_transform(data[\"ticker\"])\n",
        "\n",
        "# Initialize scalers\n",
        "scaler_ticker = StandardScaler()\n",
        "scaler_technical = StandardScaler()\n",
        "scaler_future_price = StandardScaler()\n",
        "\n",
        "# Collect values\n",
        "log_return_vals = data[['log_return_30d']].values\n",
        "stock_ticker = data[['encoded_ticker']].values\n",
        "stock_technical = data.filter([\"rsi\", \"cdl2crows\"]).values\n",
        "\n",
        "# Scale values\n",
        "data['scaled_log_return_30d'] = scaler_future_price.fit_transform(log_return_vals)\n",
        "data['scaled_ticker'] = scaler_ticker.fit_transform(stock_ticker)\n",
        "scaled_technical = scaler_technical.fit_transform(stock_technical)\n",
        "\n",
        "\n",
        "# Unpack necessary scaled vals\n",
        "data['scaled_rsi'] = scaled_technical[:, 0]\n",
        "data['scaled_cdl2crows'] = scaled_technical[:, 1]\n",
        "\n",
        "# Group the data by ticker\n",
        "grouped_dfs = data.groupby('ticker')\n",
        "grouped_dfs = {ticker: df.sort_values(by='date').reset_index(drop=True) for ticker, df in grouped_dfs}\n",
        "\n",
        "# Save scalers\n",
        "joblib.dump(scaler_ticker, \"scaler_ticker.pkl\")\n",
        "joblib.dump(scaler_technical, \"scaler_technical.pkl\")\n",
        "joblib.dump(scaler_future_price, \"scaler_future_price.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDEglV1Qfwtq"
      },
      "outputs": [],
      "source": [
        "# Your features\n",
        "feature_cols = ['scaled_close', 'scaled_rsi', 'scaled_macd', 'scaled_sma_10', 'scaled_sma_30',\n",
        "                'scaled_ticker', 'scaled_macdsignal', 'scaled_macdhist']\n",
        "\n",
        "# Function for a single ticker\n",
        "def process_ticker(ticker_df_pair):\n",
        "    ticker, df = ticker_df_pair\n",
        "    x_local, y_local = [], []\n",
        "\n",
        "    if len(df) < 91:\n",
        "        return x_local, y_local\n",
        "\n",
        "    for i in range(60, len(df) - 30):\n",
        "        window = df.iloc[i - 60:i][feature_cols].values\n",
        "        x_local.append(window)\n",
        "        y_local.append(df.iloc[i]['scaled_log_return_30d'])\n",
        "\n",
        "    return x_local, y_local\n",
        "\n",
        "# Needed to prevent issues with multiprocessing in Colab\n",
        "def run_parallel_windowing(grouped_dfs):\n",
        "    x_train, y_train = [], []\n",
        "\n",
        "    with ProcessPoolExecutor(max_workers=multiprocessing.cpu_count()) as executor:\n",
        "        futures = [executor.submit(process_ticker, item) for item in grouped_dfs.items()]\n",
        "        for f in tqdm(as_completed(futures), total=len(futures), desc=\"Processing tickers\"):\n",
        "            x_result, y_result = f.result()\n",
        "            x_train.extend(x_result)\n",
        "            y_train.extend(y_result)\n",
        "\n",
        "    return np.array(x_train), np.array(y_train)\n",
        "\n",
        "# Call it like this:\n",
        "x_train, y_train = run_parallel_windowing(grouped_dfs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgaQ4tDHfwtq"
      },
      "outputs": [],
      "source": [
        "with tf.device('/device:GPU:0'):\n",
        "\n",
        "  model = Sequential()\n",
        "\n",
        "  # Convolutional layers for local pattern extraction\n",
        "  model.add(Conv1D(filters=64, kernel_size=3, padding='same', activation='relu', input_shape=(x_train.shape[1], x_train.shape[2])))\n",
        "  model.add(Conv1D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
        "  model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "  # Stacked Bidirectional LSTM for capturing sequence relationships\n",
        "  model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(Bidirectional(LSTM(64)))\n",
        "  model.add(Dropout(0.3))\n",
        "\n",
        "  # Dense layers for final nonlinear transformation\n",
        "  model.add(Dense(128, kernel_regularizer=regularizers.l2(0.001)))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(LeakyReLU(alpha=0.1))\n",
        "  model.add(Dropout(0.4))  # Slightly increased dropout to reduce overfitting\n",
        "\n",
        "  model.add(Dense(64, kernel_regularizer=regularizers.l2(0.001)))\n",
        "  model.add(LeakyReLU(alpha=0.1))\n",
        "  model.add(Dropout(0.3))\n",
        "\n",
        "  model.add(Dense(1))  # Output log return prediction\n",
        "\n",
        "  model.summary()\n",
        "  model.compile(\n",
        "      optimizer=keras.optimizers.Adam(learning_rate=0.001, clipnorm=1.0),\n",
        "      loss=keras.losses.Huber(delta=1.0),  # Huber = better for stability on noisy targets\n",
        "      metrics=[keras.metrics.RootMeanSquaredError()]\n",
        "  )\n",
        "\n",
        "\n",
        "  lr_schedule = keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
        "                                                  factor=0.5,\n",
        "                                                  patience=3,\n",
        "                                                  verbose=1)\n",
        "\n",
        "  early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                                            patience=10,\n",
        "                                            restore_best_weights=True)\n",
        "\n",
        "  training = model.fit(\n",
        "      x_train, y_train,\n",
        "      epochs=200,                # Max number of epochs\n",
        "      batch_size=128,\n",
        "      validation_split=0.1,      # Use part of training data for validation\n",
        "      callbacks=[early_stopping]\n",
        "  )\n",
        "  model.save(\"model4-5.keras\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}