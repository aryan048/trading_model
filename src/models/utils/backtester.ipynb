{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from models.utils.sp_scraper import scrape_sp500_symbols\n",
    "import yfinance as yf\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from tensorflow import keras\n",
    "import talib as ta\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import joblib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('/Users/aryanhazra/Downloads/VSCode Repos/trading_model/src/models/model3/model3.keras')\n",
    "\n",
    "# Load all scalers\n",
    "scaler_future_price = joblib.load('/Users/aryanhazra/Downloads/VSCode Repos/trading_model/src/models/model3/scaler_future_price.pkl')\n",
    "scaler_ticker = joblib.load('/Users/aryanhazra/Downloads/VSCode Repos/trading_model/src/models/model3/scaler_ticker.pkl')\n",
    "scaler_technical = joblib.load('/Users/aryanhazra/Downloads/VSCode Repos/trading_model/src/models/model3/scaler_technical.pkl')\n",
    "\n",
    "# Initialize the scalers dictionary\n",
    "scalers = {}\n",
    "\n",
    "# Path to the scalers directory\n",
    "scalers_dir = '/Users/aryanhazra/Downloads/VSCode Repos/trading_model/src/models/model3/scalers'\n",
    "\n",
    "# Loop through each ticker directory\n",
    "for ticker in os.listdir(scalers_dir):\n",
    "    ticker_path = os.path.join(scalers_dir, ticker)\n",
    "    if os.path.isdir(ticker_path):\n",
    "        # Load all three scalers for this ticker\n",
    "        scaler_close = joblib.load(os.path.join(ticker_path, 'scaler_close.pkl'))\n",
    "        scaler_sma_10 = joblib.load(os.path.join(ticker_path, 'scaler_sma_10.pkl'))\n",
    "        scaler_sma_30 = joblib.load(os.path.join(ticker_path, 'scaler_sma_30.pkl'))\n",
    "        \n",
    "        # Store them in a dictionary\n",
    "        scalers[ticker] = {\n",
    "            'scaler_close': scaler_close,\n",
    "            'scaler_sma_10': scaler_sma_10,\n",
    "            'scaler_sma_30': scaler_sma_30\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace '.' with '-' in ticker symbols, also add SPY as a benchmark\n",
    "sp_tickers = [ticker.replace(\".\", \"-\") for ticker in sorted(scrape_sp500_symbols())] + [\"^GSPC\"]\n",
    "data_frames = []\n",
    "\n",
    "def process_ticker(ticker):\n",
    "    try:\n",
    "        # Retry logic\n",
    "        while True:\n",
    "            try:\n",
    "                ticker_data = yf.Ticker(ticker).history(period=\"max\")\n",
    "                break\n",
    "            except Exception:\n",
    "                time.sleep(10)\n",
    "\n",
    "        if ticker_data.empty:\n",
    "            return None\n",
    "\n",
    "        # Process data\n",
    "        ticker_data.reset_index(inplace=True)\n",
    "        ticker_data.columns = ticker_data.columns.str.lower()\n",
    "\n",
    "        ticker_data['ticker'] = ticker\n",
    "        ticker_data['log_return_30d'] = np.log(ticker_data['close'].shift(-30) / ticker_data['close'])\n",
    "\n",
    "        ticker_data['rsi'] = ta.RSI(ticker_data['close'], timeperiod=14)\n",
    "        macd, macdsignal, macdhist = ta.MACD(ticker_data['close'], fastperiod=12, slowperiod=26, signalperiod=9)\n",
    "        ticker_data['macd'] = macd\n",
    "        ticker_data['sma_10'] = ta.SMA(ticker_data['close'], timeperiod=10)\n",
    "        ticker_data['sma_30'] = ta.SMA(ticker_data['close'], timeperiod=30)\n",
    "\n",
    "        # Select and scale\n",
    "        close_vals = ticker_data[['close']].values\n",
    "        sma_10_vals = ticker_data[['sma_10']].values\n",
    "        sma_30_vals = ticker_data[['sma_30']].values\n",
    "\n",
    "        ticker_data['scaled_close'] = scalers[ticker]['scaler_close'].transform(close_vals)\n",
    "        ticker_data['scaled_sma_10'] = scalers[ticker]['scaler_sma_10'].transform(sma_10_vals)\n",
    "        ticker_data['scaled_sma_30'] = scalers[ticker]['scaler_sma_30'].transform(sma_30_vals)\n",
    "\n",
    "        return ticker_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {ticker}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Use multithreading for I/O-bound operations like data download\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    futures = {executor.submit(process_ticker, ticker): ticker for ticker in sp_tickers}\n",
    "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Downloading data\", unit=\"ticker\"):\n",
    "        result_data = future.result()\n",
    "        if result_data is not None:\n",
    "            data_frames.append(result_data)\n",
    "        else:\n",
    "            print(f\"Failed to process {future.result()}\")\n",
    "\n",
    "# Combine all dataframes\n",
    "data = pd.concat(data_frames, ignore_index=True)\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for the model\n",
    "# Label encode the ticker column\n",
    "label_encoder = LabelEncoder()\n",
    "data[\"encoded_ticker\"] = label_encoder.fit_transform(data[\"ticker\"])\n",
    "\n",
    "# Scale future price\n",
    "log_return_vals = data[['log_return_30d']].values\n",
    "data['scaled_log_return_30d'] = scaler_future_price.transform(log_return_vals)\n",
    "\n",
    "# Scale the ticker column\n",
    "stock_ticker = data.filter([\"encoded_ticker\"])\n",
    "stock_ticker = stock_ticker.values\n",
    "scaled_ticker = scaler_ticker.transform(stock_ticker)\n",
    "data['scaled_ticker'] = scaled_ticker\n",
    "\n",
    "#scale technical columns\n",
    "stock_technical = data.filter([\"return\", \"rsi\", \"macd\"])\n",
    "stock_technical = stock_technical.values\n",
    "scaled_technicals = scaler_technical.transform(stock_technical)\n",
    "# Insert scaled data into the original dataframe\n",
    "data['scaled_rsi'] = scaled_technicals[:, 0]\n",
    "data['scaled_macd'] = scaled_technicals[:, 1]\n",
    "\n",
    "# Group the data by ticker\n",
    "grouped_dfs = data.groupby('ticker')\n",
    "grouped_dfs = {ticker: df.sort_values(by='date').reset_index(drop=True) for ticker, df in grouped_dfs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your list of tuples is called ticker_df_list\n",
    "spy_data = next(df for ticker, df in grouped_dfs.items() if ticker == \"^GSPC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['scaled_close', 'scaled_rsi', 'scaled_macd', 'scaled_sma_10', 'scaled_sma_30', 'scaled_ticker']\n",
    "# Start on day 60 of SPY\n",
    "# Initalize compound returns\n",
    "predicted_compound_return = 1.0  # Starting with 1 (100%)\n",
    "real_compound_return = 1.0\n",
    "spy_real_compound_return = 1.0\n",
    "\n",
    "# Have start date for backtesting\n",
    "start_date = pd.to_datetime('2000-01-01').tz_localize(None)  # Make start_date timezone-naive\n",
    "spy_data['date'] = pd.to_datetime(spy_data['date']).dt.tz_localize(None)  # Make spy_data dates timezone-naive\n",
    "start_idx = spy_data['date'].sub(start_date).abs().idxmin()\n",
    "\n",
    "for i in tqdm(range(start_idx, len(spy_data) - 30, 30), desc=\"Processing SPY data\", unit=\"step\"):\n",
    "    target_date = spy_data.iloc[i]['date']\n",
    "    x_test = []\n",
    "    y_real = []\n",
    "    predictions = {}\n",
    "    for ticker, df in grouped_dfs.items():\n",
    "        # Ensure target_date is timezone-naive\n",
    "        target_date = pd.to_datetime(target_date).replace(tzinfo=None)\n",
    "\n",
    "        # Ensure df['date'] is also timezone-naive\n",
    "        df['date'] = pd.to_datetime(df['date']).dt.tz_localize(None)\n",
    "\n",
    "        # Now this check will work correctly\n",
    "        if target_date not in df['date'].values:\n",
    "            continue\n",
    "\n",
    "        date_idx = df.index[df['date'] == target_date][0]\n",
    "\n",
    "        # Check if there are 60 days before and 30 after\n",
    "        if not (date_idx >= 60 and date_idx + 30 < len(df)):\n",
    "            continue\n",
    "\n",
    "        window = df.iloc[date_idx - 60:date_idx][feature_cols].values  # shape (60, num_features)\n",
    "\n",
    "        x_test.append([ticker, window])\n",
    "\n",
    "        # Predict the \"price in 30 days\" from the current i-th index (i.e. day 60 of the window)\n",
    "        y_real.append(df.iloc[date_idx]['log_return_30d'])\n",
    "\n",
    "    #predicting based on the second vals of x_test (the windows)\n",
    "    predictions = model.predict(np.array([x[1] for x in x_test]))\n",
    "    #inverting the scaling\n",
    "    predictions = scaler_future_price.inverse_transform(predictions)\n",
    "    # Assigning predictions, and real vals based on ticker\n",
    "    predictions = {x_test[i][0]: (float(predictions[i][0]), float(y_real[i])) for i in range(len(predictions))}\n",
    "    #sorting the predictions by the first value (the predicted vals)\n",
    "    top_10_predictions = dict(sorted(predictions.items(), \n",
    "                                        key=lambda x: x[1][0], \n",
    "                                        reverse=True)[:10])\n",
    "    clear_output(wait=True)  # The wait=True parameter prevents flickering\n",
    "    print(f\"\\nPredictions for date: {target_date}\")\n",
    "    print(\"Top 10 Predicted Returns:\")\n",
    "    print(\"Ticker | Predicted Return | Actual Return\")\n",
    "    print(\"-\" * 45)\n",
    "    for ticker, (pred, actual) in top_10_predictions.items():\n",
    "        # Convert to percentages by multiplying by 100\n",
    "        print(f\"{ticker:6} | {pred*100:13.2f}% | {actual*100:12.2f}%\")\n",
    "\n",
    "    # Convert average returns to percentages\n",
    "    avg_predicted_return = np.mean([pred for _, (pred, _) in top_10_predictions.items()])\n",
    "    print(\"\\nAverage Predicted Return for Top 10: {:.2f}%\".format(avg_predicted_return*100))\n",
    "\n",
    "    avg_real_return = np.mean([actual for _, (_, actual) in top_10_predictions.items()])\n",
    "    print(\"\\nAverage Actual Return for Top 10: {:.2f}%\".format(avg_real_return*100))\n",
    "\n",
    "    # For compound returns, we'll show the total percentage gain/loss\n",
    "    predicted_compound_return *= np.exp(avg_predicted_return)\n",
    "    print(\"Predicted Compound Return: {:.2f}%\".format((predicted_compound_return-1)*100))\n",
    "\n",
    "    real_compound_return *= np.exp(avg_real_return)\n",
    "    print(\"Real Compound Return: {:.2f}%\".format((real_compound_return-1)*100))\n",
    "\n",
    "    spy_real_compound_return *= np.exp(spy_data.iloc[i]['log_return_30d'])\n",
    "    print(\"SPY Real Compound Return: {:.2f}%\".format((spy_real_compound_return-1)*100))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
